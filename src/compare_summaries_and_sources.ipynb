{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2df1298-ac27-498b-aed0-963302517eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import seaborn as sns\n",
    "from scipy import spatial\n",
    "\n",
    "!pip install SentencePiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f25ba7-922b-4b8c-aa8b-68e4942be901",
   "metadata": {},
   "source": [
    "## Load and prepare the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab578dc3-7534-43f8-b805-18b0474649a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path.cwd().parent / 'data'\n",
    "SUMM_FOLDER = DATA / 'summaries_finetune'\n",
    "TEXT_FILES = SUMM_FOLDER / 'text_files_copy'\n",
    "SOURCE_TEXTS = SUMM_FOLDER / 'source_texts_clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b639158-e4ca-4158-b2b1-f5f84e8e5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df = pd.read_csv(SUMM_FOLDER / 'final_summaries_ai_aloe_fixed.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff53bc1-59c6-4a94-ae18-ffad2793b0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "df = summaries_df[['text', 'source', 'Main.Point', 'Details']]\n",
    "ds = Dataset.from_pandas(df.reset_index(drop=True))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609dc179-dca0-4a08-ba27-aa065d44e1f6",
   "metadata": {},
   "source": [
    "## Run the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "854f5641-14af-411e-86f6-6e23c77450e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BigBirdTokenizer, DataCollatorWithPadding\n",
    "from transformers import LongformerTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "model_name = \"allenai/longformer-base-4096\" #\"google/bigbird-roberta-base\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name, output_hidden_states=True).to(DEVICE)\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28b10ec6-c64b-4c5e-ae00-ca418795349d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the device\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666cc7fd-dcfc-4b79-8bef-c54f0af50670",
   "metadata": {},
   "source": [
    "### Run the transformer operation in cuda, then detach results and compare to make list of cos similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71190ca-a7ff-4bac-840f-3544c0ae09d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2662\r"
     ]
    }
   ],
   "source": [
    "def getLastState(text):\n",
    "    tokenized_text = tokenizer(text, return_tensors='pt').to(DEVICE)\n",
    "    outputs = model(**tokenized_text)\n",
    "    return outputs.last_hidden_state[0][0]\n",
    "\n",
    "cos_similarities = []\n",
    "counter = 0\n",
    "for row in df.iterrows():\n",
    "    print(counter, end='\\r')\n",
    "    text = row[1]['text']\n",
    "    source = row[1]['source']\n",
    "    text_embedding = getLastState(text).cpu().detach().numpy()\n",
    "    source_embedding = getLastState(source).cpu().detach().numpy()\n",
    "    cos_similarities.append(1 - spatial.distance.cosine(text_embedding, source_embedding))\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a28d09c-1702-4846-8fc5-c0b381e1faab",
   "metadata": {},
   "source": [
    "### Attach the list to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ee2d8-73f2-4efe-a5ed-f945fc698e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cos_similarity'] = cos_similarities\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0d275-a566-444e-804a-e821be1514f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_doc_embed(dataset, source=None):\n",
    "#     # make sure the correct source (prompt/text) is tokenized\n",
    "#     tokenization = lambda x: tokenizer(x[source], padding=True, return_tensors='pt').to(DEVICE)\n",
    "\n",
    "#     dataset = dataset.map(tokenization, batched=True)\n",
    "#     dataset = dataset.with_format('torch',\n",
    "#                                   columns=['input_ids', 'attention_mask'],\n",
    "#                                   device=DEVICE)\n",
    "    \n",
    "#     # load data efficiently\n",
    "#     loader = torch.utils.data.DataLoader(dataset,\n",
    "#                                          batch_size=16,\n",
    "#                                          collate_fn=data_collator)\n",
    "    \n",
    "#     features = np.empty([0,768])\n",
    "#     for batch in tqdm(loader):\n",
    "#         batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "#         with torch.no_grad():\n",
    "#             cls_tok = model(batch['input_ids'],\n",
    "#                             batch['attention_mask']).last_hidden_state[:,0,:]\n",
    "#             features = np.vstack((features,\n",
    "#                                   cls_tok.cpu().numpy()))\n",
    "            \n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced662d6-6f37-40f4-b095-7cc99b96902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_sample = ds#.shuffle().select(range(100))\n",
    "\n",
    "# prompt_embeds = get_doc_embed(ds_sample, source='prompt')\n",
    "# text_embeds = get_doc_embed(ds_sample, source='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cde063-e861-493d-8a37-e72f5a274603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import spatial\n",
    "# cos_similarities = []\n",
    "# for text_embed, prompt_embed in zip(text_embeds, prompt_embeds):\n",
    "#     cos_similarities.append(1 - spatial.distance.cosine(text_embed, prompt_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272b644-20ab-4274-827d-f4e1fe12ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summaries_df['cos_similarity'] = pd.Series(cos_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13f649-d7d8-49d8-8bde-527588a45ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import spatial\n",
    "\n",
    "# def getSimilarity(df):\n",
    "#     text_embedding = getLastState(ds['text']).detach().numpy()\n",
    "#     prompt_embedding = getLastState(df['prompt']).detach().numpy()\n",
    "#     return 1 - spatial.distance.cosine(text_embedding, prompt_embedding)\n",
    "\n",
    "# df['cos_similarity'] = df.apply(lambda x: getSimilarity(x), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ee1c1-6aee-4bb0-9580-dca2f1e02b93",
   "metadata": {},
   "source": [
    "## Examine the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91f811f-76f7-4ab5-9326-5b9ad467b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of words\n",
    "df['num_words'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Adjust the window for cosine similarity\n",
    "min_similarity = 0\n",
    "max_similarity = 1\n",
    "df1 = df[df['cos_similarity'] > min_similarity]\n",
    "df1 = df1[df1['cos_similarity'] < max_similarity]\n",
    "\n",
    "print('Correlation with number of words:\\n', df1[['cos_similarity', 'num_words']].corr(), '\\n')\n",
    "print('Correlation with main point:\\n', df1[['cos_similarity', 'Main.Point']].corr(), '\\n')\n",
    "print('Correlation with details:\\n', df1[['cos_similarity', 'Details']].corr(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5112ce73-7bfc-494c-b5e1-7ddc6ace3f75",
   "metadata": {},
   "source": [
    "### Chart the correlation between cosine similarity and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26a0ab-a13a-4a84-b0ac-1d4579b150a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(df1[df1['cos_similarity'] > 0][['cos_similarity', 'Main.Point']].corr())\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "sns.regplot(data=df1[df1['cos_similarity'] > 0], x='cos_similarity', y='Main.Point', ax = axs[0])\n",
    "#plt.title('Correlation between Cosine Similarity and Main Point')\n",
    "plt.ylabel('Main Point')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "axs[0].set_title('Cosine Similarity and Main Point')\n",
    "\n",
    "sns.regplot(data=df1, x='cos_similarity', y='Details', ax = axs[1])\n",
    "#plt.title('Correlation between Cosine Similarity and Details')\n",
    "plt.ylabel('Details')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "axs[1].set_title('Cosine Similarity and Details')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f3a0d-b0b8-4b9e-96e8-36d0cd7eadd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df1, x='cos_similarity')\n",
    "plt.title('Histogram of Cosine Similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fb338f-a5b3-430a-a069-3c1dae370681",
   "metadata": {},
   "source": [
    "### Examine the texts themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18283bd6-561e-445d-8682-a2034c495b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(df1))\n",
    "df1_sorted = df1.sort_values(by='cos_similarity').reset_index()\n",
    "\n",
    "i = 1500\n",
    "\n",
    "text = df1_sorted.iloc[i]['text']\n",
    "sim = df1_sorted.iloc[i]['cos_similarity']\n",
    "print('SUMMARY', i, '- SIMILARITY TO SOURCE:', sim, '\\n' + text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
