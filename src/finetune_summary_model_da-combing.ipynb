{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f4be4b-5a3e-4b90-8884-8c86f1037202",
   "metadata": {},
   "source": [
    "# Finetune the Summary Grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc012f-d9ce-454c-8c26-4299646a49ff",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21258e8d-edd4-4723-8240-4766751e99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install SentencePiece\n",
    "!pip install \"ray[tune]\"\n",
    "!pip install wandb\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(font='Liberation Serif',\n",
    "              rc={'figure.figsize': (7.5,3.75),\n",
    "                  'font.size': 11,\n",
    "                  'figure.dpi': 300,\n",
    "                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6050e15f-9efa-445a-8c8b-4f8906c49662",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path.cwd().parent / 'data'\n",
    "SUMM_FOLDER = DATA / 'summaries_finetune'\n",
    "TEXT_FILES = SUMM_FOLDER / 'text_files_copy'\n",
    "SOURCE_TEXTS = SUMM_FOLDER / 'source_texts'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ac55c-5818-4434-93a5-a7cf77ffa122",
   "metadata": {},
   "source": [
    "## Load and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2160889-7b00-4443-a7ab-dd71118c8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df = pd.read_csv(SUMM_FOLDER / 'final_summaries_ai_aloe_fixed.csv')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "  \n",
    "# copy the data\n",
    "df_normalized = summaries_df.copy()\n",
    "  \n",
    "# apply normalization techniques\n",
    "df_normalized['content_pca'] = MinMaxScaler().fit_transform(np.array(df_normalized['content_pca']).reshape(-1,1))\n",
    "df_normalized['paraphrase_pca'] = MinMaxScaler().fit_transform(np.array(df_normalized['paraphrase_pca']).reshape(-1,1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c074d42f-1a98-4340-8bd6-3f2d88907897",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized['text'] = df_normalized['text'] + '</s>' + df_normalized['source']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1073ed-3744-4ad7-a97a-6ec852e15c54",
   "metadata": {},
   "source": [
    "### Seperate out a test set to avoid prompt effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "269955cc-ef88-4bc2-8a04-624d30ba1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_texts = df_normalized['source_text_filename_clean'].value_counts().to_frame().reset_index()\n",
    "texts_to_remove = list(source_texts.iloc[15:31]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4b909a-6e93-40db-b517-e6db448e00e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test n: 703\n",
      "train n: 3987\n"
     ]
    }
   ],
   "source": [
    "# source_texts = df_normalized['source_text'].value_counts().to_frame()\n",
    "# texts_to_remove = list(source_texts.iloc[1:6].index)\n",
    "\n",
    "test_df = df_normalized[df_normalized['source_text_filename_clean'].isin(texts_to_remove)]\n",
    "train_df = df_normalized[df_normalized['source_text_filename_clean'].isin(texts_to_remove) == False]\n",
    "print('test n:', len(test_df))\n",
    "print('train n:', len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631dbfab-201f-4f62-978a-c031f80dde42",
   "metadata": {},
   "source": [
    "## Transformer time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1122fb77-194d-4e32-8184-417cd1f5d2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset, Value, ClassLabel, Features, DatasetDict\n",
    "# from transformers import LongformerTokenizer, LongformerForSequenceClassification, LongformerConfig\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaConfig, DataCollatorWithPadding, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification, LongformerConfig\n",
    "\n",
    "\n",
    "import torch\n",
    "seed = 42\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name, padding=True, model_max_length = 1024)\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c0386-b00b-46b6-a76d-2fbc3b19f688",
   "metadata": {},
   "source": [
    "### Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f836f89c-97d1-449d-944e-11a61c18f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataset(df):\n",
    "    full_dataset = Dataset.from_pandas(df, preserve_index=False)\n",
    "    # 70% train, 30% test\n",
    "    train_valid = full_dataset.train_test_split(test_size=0.176, seed=seed)\n",
    "    # gather everyone if you want to have a single DatasetDict\n",
    "    final_dataset = DatasetDict({\n",
    "        'train': train_valid['train'],\n",
    "        'valid': train_valid['test']})\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc5499d-2a60-4707-845e-ac4ac6a702aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df = train_df[['text', 'content_pca']]\n",
    "content_df.columns = ['text', 'labels']\n",
    "content_ds = buildDataset(content_df)\n",
    "\n",
    "paraphrase_df = train_df[['text', 'paraphrase_pca']]\n",
    "paraphrase_df.columns = ['text', 'labels']\n",
    "paraphrase_ds = buildDataset(paraphrase_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4650f96e-0fbb-4b69-9c0e-e33f4f5c0903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this adds the test set in that we seperated earlier\n",
    "content_ds['test'] = Dataset.from_pandas(test_df[['text', 'content_pca']].rename(columns={'content_pca':'labels', 'text':'text'}), preserve_index=False)\n",
    "paraphrase_ds['test'] = Dataset.from_pandas(test_df[['text', 'paraphrase_pca']].rename(columns={'paraphrase_pca':'labels', 'text': 'text'}), preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5753ef83-3dbb-4c43-8bb7-6809279436df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 3285\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 702\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 703\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "774e2c55-3579-4810-b6d2-e358ffccc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_name\n",
    "tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c1ab814-0ff6-4389-a330-398e531cd3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2086952e76b45dbb3d451600e8c15ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0135b9443d40d4aaf57e01d6b0a03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd936669e8a4580be83a1c524f59f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d5ea73c29e41a7b356f12000a68a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718b7fc25b8f46468edc7e9aeefa6fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ceb424176740d5abcc0e766aabc4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize them\n",
    "def tokenize_inputs(example):\n",
    "    return tokenizer(example['text'], truncation = True)\n",
    "\n",
    "content_ds_t = content_ds.map(tokenize_inputs, batched=True)\n",
    "paraphrase_ds_t = paraphrase_ds.map(tokenize_inputs, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fe3d898-5fcb-43aa-b923-5c23869fceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in content_ds_t['train']['input_ids']:\n",
    "    if len(i) > 1024:\n",
    "        print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a123df09-9536-4171-953d-ef4ffb2c1ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Food chains are what ecologists use to to show the distribution of energy and matter in an ecosystem. It is in the shape of a pyramid. The bottom is called the autotroph, and is energy from the sun. Energy flows from here to the top, but decreases at each subsequent level. The energy that is lost at each level becomes heat. Population size is often how pyramids are stacked, but that is not always the case. A pyramid of biomass expresses the weight ofliving material at each level.\\n</s>Energy and trophic levels: Ecological pyramids \\n How can you show how energy is used in an ecosystem? Ecologists use food chains and food webs to model the distribution of matter and energy within an ecosystem. They also use another kind of model, called and ecological pyramid. An ecological pyramid shows how energy flows through an ecosystem. The base of the ecological pyramid represents the autotrophs, or first trophic level. Higher trophic levels are layered on top of one another. The initial source of energy for all ecological pyramids is energy from the sun. \\n Energy decreases at each succeeding trophic level. The total energy transfer from one trophic level to the next is only about ten percent because organisms fail to capture and eat all the food available at the trophic level below them. When an organism consumes food, it uses some of the energy in the food for metabolism, some for building body tissues, and some is given off as waste. When the organism is eaten, the energy that was used to build body tissue is available as energy to be used by the organism that consumed it. The energy lost at each successive trophic level enters the environment as heat.\\n Ecologists construct a pyramid of numbers based on the population sizes of organisms in each trophic level. Sometimes population sizes decrease at each higher trophic level. This is not always true. For example, one tree can be food for 50,000 insects. In this case, the pyramid would be inverted.\\n A pyramid of biomass expresses the weight of living material at each trophic level. Ecologists calculate the biomass at each trophic level by finding the average weight of each species at that trophic level and multiplying by the estimated number of organisms in each population.\\n</s>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(content_ds_t['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f68cf4-6412-4d1b-ac35-36a2441b691d",
   "metadata": {},
   "source": [
    "### Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a09f01ba-3e92-4511-b414-282c600a0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr \n",
    "\n",
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    mse = mean_squared_error(labels, logits)\n",
    "    rmse = mean_squared_error(labels, logits, squared=False)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    r2 = r2_score(labels, logits)\n",
    "    smape = 1/len(labels) * np.sum(2 * np.abs(logits-labels) / (np.abs(labels) + np.abs(logits))*100)\n",
    "\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"smape\": smape}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324a1235-bc9a-4f32-b965-fa1927b40ef7",
   "metadata": {},
   "source": [
    "### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6a2745a-31a6-4cf8-9310-1e7c3a48f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-05\n",
    "batch_size = 8\n",
    "seed = 42\n",
    "num_epochs = 6\n",
    "#da_model_name = './results/checkpoint-78500'\n",
    "\n",
    "# def model_init():\n",
    "#     return RobertaForSequenceClassification.from_pretrained(model_name,\n",
    "#                                                               num_labels=1).to(device)\n",
    "def model_init():\n",
    "    return LongformerForSequenceClassification.from_pretrained(model_name, num_labels=1).to(device)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1ee81-659f-4f75-a385-f90e7896537e",
   "metadata": {},
   "source": [
    "### Train the content model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b9b50-e943-4da0-9316-0cf71866e7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /home/jovyan/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
      "Model config LongformerConfig {\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/allenai/longformer-base-4096/resolve/main/pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/transformers/a7a586602e625bd012d75abdfcc615f5bb1fe133273845f7381332c634273bd9.dc3a4f03d4ab11f972b126d0e6b67f43e5d9003b3aec54f8e549cc7e2d42398d\n",
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtiedaar1\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/shared/2022_09_21_textbook/src/wandb/run-20221111_204347-3tr40py1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tiedaar1/huggingface/runs/3tr40py1\" target=\"_blank\">./results/longformer_content_checkpoints</a></strong> to <a href=\"https://wandb.ai/tiedaar1/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='599' max='2466' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 599/2466 10:51 < 33:57, 0.92 it/s, Epoch 1.45/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>R2</th>\n",
       "      <th>Smape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.022626</td>\n",
       "      <td>0.022626</td>\n",
       "      <td>0.150418</td>\n",
       "      <td>0.122235</td>\n",
       "      <td>0.451677</td>\n",
       "      <td>20384.606838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f'./results/longformer_content_checkpoints',\n",
    "    optim = 'adamw_torch',\n",
    "    num_train_epochs = num_epochs,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    weight_decay = 0.01,\n",
    "    learning_rate = learning_rate,\n",
    "    logging_dir = f'./logs/content',\n",
    "    save_total_limit = 10,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'mse',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\", \n",
    "    greater_is_better = False,\n",
    "    seed=seed,\n",
    "    log_level = 'error',  # took me ages to find these options\n",
    "    disable_tqdm = False, # enable output cell scrolling in JupyterLab for even more beautiful output :D\n",
    ") \n",
    "\n",
    "    # Call the Trainer\n",
    "content_trainer = Trainer(\n",
    "    model_init = model_init,\n",
    "    args = training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset = content_ds_t['train'],\n",
    "    eval_dataset = content_ds_t['valid'],\n",
    "    compute_metrics = compute_metrics_for_regression,\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "content_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3725ba63-7082-409b-83ef-6c81bd473405",
   "metadata": {},
   "source": [
    "### Train the paraphrase model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16194a-1452-48da-ba03-bdb2d7fde56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f'./results/longformer_paraphrase_checkpoints',\n",
    "    optim = 'adamw_torch',\n",
    "    num_train_epochs = num_epochs,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    weight_decay = 0.01,\n",
    "    learning_rate = learning_rate,\n",
    "    logging_dir = f'./logs/paraphrase',\n",
    "    save_total_limit = 10,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'mse',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    greater_is_better = False,\n",
    "    seed=seed,\n",
    "    log_level = 'error', # took me ages to find these options\n",
    "    disable_tqdm = False, # enable output cell scrolling in JupyterLab for even more beautiful output :D\n",
    ") \n",
    "\n",
    "    # Call the Trainer\n",
    "paraphrase_trainer = Trainer(\n",
    "    model_init = model_init,\n",
    "    args = training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset = paraphrase_ds_t['train'],\n",
    "    eval_dataset = paraphrase_ds_t['valid'],\n",
    "    compute_metrics = compute_metrics_for_regression,\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "paraphrase_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09d479-5c97-4f4b-897f-590611fc38e7",
   "metadata": {},
   "source": [
    "## Evaluate the models on the test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ba88e-c412-40bc-8827-39cfacd29181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "con_preds, con_labs, con_metrics = content_trainer.predict(content_ds_t['test'])\n",
    "con_actual = content_ds_t['test']['labels']\n",
    "print(scipy.stats.pearsonr(con_actual, con_preds))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "con_preds.flatten()\n",
    "plt.scatter(con_preds, con_actual, alpha=0.5)\n",
    "plt.ylabel('true content score')\n",
    "plt.xlabel('predicted content score')\n",
    "plt.title('Content Model Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e6a06d-bc89-4904-99ad-46f5387703e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_preds, para_labs, para_metrics = paraphrase_trainer.predict(paraphrase_ds_t['test'])\n",
    "paraphrase_actual = paraphrase_ds_t['test']['labels']\n",
    "print(scipy.stats.pearsonr(paraphrase_actual, para_preds))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "para_preds.flatten()\n",
    "plt.scatter(para_preds, paraphrase_actual, alpha=0.5)\n",
    "plt.ylabel('true paraphrase score')\n",
    "plt.xlabel('predicted paraphrase score')\n",
    "plt.title('Paraphrase Model Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde87fd-00b9-4cfb-8471-ff6f7c7ea4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paraphrase_trainer.save_model(SUMM_FOLDER / 'paraphrase_model_longformer')\n",
    "# content_trainer.save_model(SUMM_FOLDER / 'content_model_longformer')\n",
    "# tokenizer.save_pretrained(SUMM_FOLDER / 'paraphrase_model_longformer/tokenizer.json')\n",
    "# tokenizer.save_pretrained(SUMM_FOLDER / 'content_model_longformer/tokenizer.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
