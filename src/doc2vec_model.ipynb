{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "634e458a-6d1c-44ec-abbf-660867aa8fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc04202-60f5-4492-a083-60f1e00bd3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path.cwd().parent / 'data'\n",
    "SUMM_FOLDER = DATA / 'summaries_finetune'\n",
    "TEXT_FILES = SUMM_FOLDER / 'text_files_copy'\n",
    "SOURCE_TEXTS = SUMM_FOLDER / 'source_texts_clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c192577-7a2a-42da-a3aa-dee41a2887cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(DATA / 'source_dict.txt', 'r')\n",
    "source_dict = json.loads(data.read())\n",
    "textbook_df = pd.DataFrame(list(source_dict.values()))\n",
    "textbook_df.columns=['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3594479-53f3-4d48-b0ff-12bb1775cb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>By the end of this section, you will be able t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>By the end of this section, you will be able t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>By the end of this section, you will be able t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>By the end of this section, you will be able t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>By the end of this section, you will be able t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4779</th>\n",
       "      <td>4685</td>\n",
       "      <td>The results from many studies indicate that vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4780</th>\n",
       "      <td>4686</td>\n",
       "      <td>People are being advised to spend less time in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4781</th>\n",
       "      <td>4687</td>\n",
       "      <td>We are thus in a situation where people are re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4782</th>\n",
       "      <td>4688</td>\n",
       "      <td>There are two types of cancers, melanoma and b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4783</th>\n",
       "      <td>4689</td>\n",
       "      <td>Tnning beds can damage are DNA. The damage to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4784 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               text\n",
       "0         0  By the end of this section, you will be able t...\n",
       "1         1  By the end of this section, you will be able t...\n",
       "2         2  By the end of this section, you will be able t...\n",
       "3         3  By the end of this section, you will be able t...\n",
       "4         4  By the end of this section, you will be able t...\n",
       "...     ...                                                ...\n",
       "4779   4685  The results from many studies indicate that vi...\n",
       "4780   4686  People are being advised to spend less time in...\n",
       "4781   4687  We are thus in a situation where people are re...\n",
       "4782   4688  There are two types of cancers, melanoma and b...\n",
       "4783   4689  Tnning beds can damage are DNA. The damage to ...\n",
       "\n",
       "[4784 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_df = pd.read_csv(SUMM_FOLDER / 'final_summaries_ai_aloe_fixed.csv', index_col=False)[['text']]\n",
    "all_text_df = pd.concat([textbook_df, summaries_df]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "794971d1-cb2c-4143-b3c1-e2c824c1054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tagged_document(list_of_list_of_words):\n",
    "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca4b14c2-568b-4b2a-b299-5eccd0d5da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokenized = all_text_df['text'].apply(lambda t: tokenize_text(t)).to_frame()\n",
    "all_tagged = all_tokenized.apply(lambda r: TaggedDocument(words=r['text'], tags='text'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "788ab52b-6a51-4ce9-9b9e-9db5a61c98fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4784/4784 [00:00<00:00, 1489426.24it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 4833907.57it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1312417.45it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 2107725.88it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1598593.88it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 3649609.01it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1631478.20it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 3708974.18it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1630550.17it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1630815.21it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1573151.73it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1612079.24it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1577976.59it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1630947.76it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1609622.20it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1649720.49it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1588721.33it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1488652.74it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1427238.80it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1631080.34it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 2328870.74it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1637202.21it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1618711.71it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1654072.24it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1591493.52it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1621065.63it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1588595.55it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1611561.35it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1059371.22it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1598593.88it/s]\n",
      "100%|██████████| 4784/4784 [00:00<00:00, 1638940.65it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample=0)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_tagged)])\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_tagged.values)]), total_examples=len(all_tagged.values), epochs=1)\n",
    "# for i in range(len(text_tokenized)):\n",
    "#     text_vector = model_dbow.infer_vector(text_tokenized.iloc[i]['text'])\n",
    "#     source_vector = model_dbow.infer_vector(source_tokenized.iloc[i]['text'])\n",
    "#     cos_similarities.append(1 - spatial.distance.cosine(text_vector, source_vector))\n",
    "# df['doc2vec_cos'] = cos_similarities\n",
    "# return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6492f9-a3dc-4edd-82fa-8a2ce87203c2",
   "metadata": {},
   "source": [
    "## Loading the model from file and using it for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1518fc39-1f35-4830-a7f8-95c6e4f99411",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install gradio\n",
    "import gradio as gr\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "988fe3bd-22ae-4f25-861d-7c7ccdbca380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6679088473320007\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Doc2Vec\n",
    "from scipy import spatial\n",
    "\n",
    "# Load the source_dict\n",
    "data = open(DATA / 'source_dict.txt', 'r')\n",
    "source_dict = json.loads(data.read())\n",
    "\n",
    "# Load the model\n",
    "model_path = '../bin/doc2vec_model'\n",
    "model = Doc2Vec.load(model_path)\n",
    "\n",
    "# A function to tokenize the text\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "# A function to get the score\n",
    "def getSimilarity(summary, chapter):\n",
    "    summary_embedding = model.infer_vector(tokenize_text(summary))\n",
    "    section_embedding = model.infer_vector(tokenize_text(chapter))\n",
    "    return 1 - spatial.distance.cosine(summary_embedding, section_embedding)\n",
    "\n",
    "\n",
    "# Here it is in practice\n",
    "section1summary = \"Economics seeks to solve the problem of scarcity, which is when human wants for goods and services exceed the available supply. A modern economy displays a division of labor, in which people earn income by specializing in what they produce and then use that income to purchase the products they need or want. The division of labor allows individuals and firms to specialize and to produce more for several reasons: a) It allows the agents to focus on areas of advantage due to natural factors and skill levels; b) It encourages the agents to learn and invent; c) It allows agents to take advantage of economies of scale. Division and specialization of labor only work when individuals can purchase what they do not produce in markets. Learning about economics helps you understand the major problems facing the world today, prepares you to be a good citizen, and helps you become a well-rounded thinker.\" \n",
    "off_topic = \"Sometimes football is not beautiful. Sometimes doing just enough, for just long enough, can lay the foundation, open the door. So it was as a counter-attack ended Australia’s agony and Denmark’s World Cup campaign. It took 60 minutes for Mat Leckie to score, an endless hour of mostly last-ditch defending and some positive moments, and a goal from Tunisia against France that meant the Socceroos would have to win or be out themselves.In the end it was Denmark’s second-half impotence that ended their tournament prematurely, their early brightness dissipating in the face of a Socceroos side which left it late but rallied when it had to and once again displayed a level of quality belying their inexperience.\"\n",
    "print(getScore(section1summary, source_dict['01-1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e67bc857-3c4d-4ff5-815a-f2950a979b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6583132743835449\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46160c14-7780-481c-9cb1-00118c0ab4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa57a1-5b20-485e-830c-c79863cb9709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# demo = gr.Interface(\n",
    "#     fn=getScore,\n",
    "#     inputs=[gr.Textbox(lines=2, placeholder=\"Summary...\"), gr.Dropdown(label = \"Chapter\", choices = list(source_dict.keys())),],\n",
    "#     outputs=[gr.Number(label = \"Wording Score\"), gr.Number(label=\"Content Score\")],\n",
    "#     title=\"Automatic Summary Scorer\",\n",
    "#     description=\"Automatic Summary Scorer for OpenStax Macroeconomics Textbook\",\n",
    "#     article=\"This is an app which provides two scores for summaries of chapters in the OpenStax textbook on Macroeconomics. The source text can be found at https://openstax.org/books/principles-macroeconomics-ap-courses-2e/pages/1-key-concepts-and-summary\"\n",
    "# )\n",
    "\n",
    "# demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
