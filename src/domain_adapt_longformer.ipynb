{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9911c7cc-39bc-4ae3-b053-d36b9d15b658",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6360b383-652f-4aa6-9bad-f8c36ddba5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/commonlit_summarize_20221028.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3030b-f301-4f99-86d0-dfd562de038f",
   "metadata": {},
   "source": [
    "## Input the source texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ae0159-7cec-42d0-8f15-92034ab94d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['https://www.commonlit.org/texts/on-tragedy',\n",
       "       'https://www.commonlit.org/texts/greek-society',\n",
       "       'https://www.commonlit.org/texts/excerpt-from-the-jungle',\n",
       "       'https://www.commonlit.org/texts/the-third-wave',\n",
       "       'https://www.commonlit.org/texts/the-nature-vs-nurture-debate',\n",
       "       'https://www.commonlit.org/texts/egyptian-social-structure'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prompt_url'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6f5571-20c9-4398-8e86-046ce6ac3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_texts = {'https://www.commonlit.org/texts/on-tragedy': \"\"\"As the sequel to what has already been said, we must proceed to consider what the poet should aim at, and what he should avoid, in constructing his plots; and by what means the specific effect of Tragedy will be produced.\n",
    "A perfect tragedy should, as we have seen, be arranged not on the simple but on the complex plan. It should, moreover, imitate actions which excite pity and fear, this being the distinctive mark of tragic imitation. It follows plainly, in the first place, that the change of fortune presented must not be the spectacle of a virtuous man brought from prosperity to adversity: for this moves neither pity nor fear; it merely shocks us. Nor, again, that of a bad man passing from adversity to prosperity: for nothing can be more alien to the spirit of Tragedy; it possesses no single tragic quality; it neither satisfies the moral sense nor calls forth pity or fear. Nor, again, should the downfall of the utter villain be exhibited. A plot of this kind would, doubtless, satisfy the moral sense, but it would inspire neither pity nor fear; for pity is aroused by unmerited misfortune, fear by the misfortune of a man like ourselves. Such an event, therefore, will be neither pitiful nor terrible. There remains, then, the character between these two extremes — that of a man who is not eminently good and just, yet whose misfortune is brought about not by vice or depravity, but by some error of judgement or frailty. He must be one who is highly renowned and prosperous — a personage like Oedipus, Thyestes, or other illustrious men. A well-constructed plot should, therefore, be single in its issue, rather than double as some maintain. The change of fortune should be not from bad to good, but, reversely, from good to bad. It should come about as the result not of vice, but of some great error or frailty, in a character either such as we have described, or better rather than worse. The practice of the stage bears out our view. At first the poets recounted any legend that came in their way. Now, the best tragedies are founded on the story of a few houses — on the fortunes of Alcmaeon, Oedipus, Orestes, Meleager, Thyestes, Telephus, and those others who have done or suffered something terrible. A tragedy, then, to be perfect according to the rules of art, should be of this construction. Hence they are in error who censure Euripides just because he follows this principle in his plays, many of which end unhappily. It is, as we have said, the right ending. The best proof is that on the stage and in dramatic competition, such plays, if well worked out, are the most tragic in effect; and Euripides, faulty though he may be in the general management of his subject, yet is felt to be the most tragic of the poets.\n",
    "In the second rank comes the kind of tragedy which some place first. Like the Odyssey, it has a double thread of plot, and also an opposite catastrophe for the good and for the bad. It is accounted the best because of the weakness of the spectators; for the poet is guided in what he writes by the wishes of his audience. The pleasure, however, thence derived is not the true tragic pleasure. It is proper rather to Comedy, where those who, in the piece, are the deadliest enemies — like Orestes and Aegisthus — quit the stage as friends at the close, and no one slays or is slain.\"\"\",\n",
    "'https://www.commonlit.org/texts/greek-society': \"\"\"Although the male citizen, with his full legal status, right to vote, hold public office, and own property, may well have dominated Greek Society, the social groups which made up the population of a typical Greek city-state or polis were remarkably diverse. Women, children, immigrants (both Greek and foreign), laborers, and slaves all had defined roles, but there was interaction (often illicit) between the classes and there was also some movement between social groups, particularly for second generation offspring and during times of stress such as wars.\n",
    "Classes\n",
    "Although the male citizen had by far the best position in Greek society, there were different classes within this group. The top of the social tree was the “best people,” the aristoi. Possessing more money than everyone else, this class could provide themselves with armor, weapons, and a horse when on military campaign. The aristocrats were often split into powerful family factions or clans who controlled all of the important political positions in the polis. Their wealth came from having property and even more importantly, the best land, i.e.: the most fertile and the closest to theprotection offered by the city walls.\n",
    "A poorer, second class of citizens existed too. These were men who had land but perhaps less productive plots and situated further from the city, their property was less well-protected than the prime land nearer the city proper. The land might be so far away that the owners had to live on it rather than travel back and forth from the city. These citizens were called the perioikoi (dwellers-round-about) or even worse “dusty-feet” and they collected together for protection in small village communities, subordinate to the neighboring city. As city populations grew and inheritances became ever more divided amongst siblings, this secondary class grew significantly.\n",
    "A third group were the middle, business class. Engaged in manufacturing, trade, and commerce, these were the nouveau riche. However, the aristoi jealously guarded their privileges and political monopoly by ensuring only landowners could rise into positions of real power. However, there was some movement between classes. Some could rise through accumulating wealth and influence; others could go down a class by becoming bankrupt (which could lead to a loss of citizenship or even being enslaved). Ill-health, losing out on an inheritance, political upheavals, or war could also result in the “best” getting their feet a little dusty.\n",
    "Women\n",
    "Female citizens had few rights in comparison to male citizens. Unable to vote, own land, or inherit, a woman’s place was in the home and her purpose in life was the rearing of children. Contact with non-family males was discouraged and women occupied their time with indoor activities such as wool-work and weaving. Spartan women were treated somewhat differently than in other states, for example, they had to do physical training (nude) like men, were permitted to own land, and could drink wine.\n",
    "Women citizens had to marry as a virgin and marriage was usually organized by the father, who chose the husband and accepted from him a dowry. If a woman had no father, then her interests (marriage prospects and property management) were looked after by a guardian (kurios), perhaps an uncle or other male relative. Married at the typical age of thirteen or fourteen, love had little to do with the matching of husband and wife. Of course, love may have developed between the couple but the best that might be hoped for was philia — a general friendship/love sentiment; eros, the love of desire, was to be found elsewhere, at least for the male. Marriages could be ended on three grounds. The first and most common was repudiation by the husband (apopempsis or ekpempsis). No reason was necessary, only the return of the dowry was expected. The second termination cause was the wife leaving the family home (apoleipsis) and in this case the woman’s new guardian was required to act as her legal representative. This was, however, a rare occurrence and the woman’s reputation in society was damaged as a result. The third ground for termination was when the bride’s father asked for his daughter back (aphairesis), probably to offer her to another man with a more attractive dowry. This last option was only possible, however, if the wife had not had children. If a woman was left a widow, she was required to marry a close male relative in order to ensure property stayed within the family.\n",
    "Women, of course, were also present in the various other non-citizen classes. The group for which we have most information is that of sex-workers. Women were here divided into two categories. The first and perhaps most common was the brothel prostitute (pornē). The second, was the higher-class prostitute (hetaira). These latter women were educated in music and culture and often formed lasting relationships with married men. It was also this class of women that entertained men (in every sense) at the celebrated symposium.\n",
    "Children & Adolescents\n",
    "Children of citizens attended schools where the curriculum covered reading, writing, and mathematics. After these basics were mastered, studies turned to literature (for example, Homer), poetry, and music (especially the lyre). Athletics was also an essential element in a young person’s education. At Sparta, boys as young as seven were grouped together under the stewardship of an older youth to be toughened up with hard physical training. In Athens, young adult citizens (aged 18-20) had to perform civil and military service and their education continued with lessons in politics, rhetoric, and culture. Girls too were educated in a similar manner to boys but with a greater emphasis on dancing, gymnastics, and musical accomplishment, which could be shown off in musical competitions and at religious festivals and ceremonies. The ultimate goal of a girl’s education was to prepare her for her role in rearing a family.\n",
    "Laborers\n",
    "Greek society included a significantly larger proportion of laborers than slaves. These were semi-free workers, wholly dependent on their employer. The most famous example is the helot class of Sparta. These dependents were not the property of a particular citizen — they could not be sold as a slave could — and they often lived with their families. Generally, they formed arrangements with their employer such as giving a quantity of their produce to the farm owner and keeping the rest for themselves. Sometimes the quota required may have been high or low, and there may also have been some extra benefits to the serfs such as protection and safety in numbers. However, the serf-class or helots could never achieve any real security as they were given little or no legal status and harshly treated, even killed in regular purges (especially in Sparta), in order to instill a fear which would ensure continued subordination to the ruling class. In certain periods such as war, helots were required to serve in the armed forces and, fighting well, they could even earn an escape from their lot and join the intermediary social groups which existed below the level of full-citizen and included such individuals as children with parents of mixed status (e.g.: father-citizen, mother-helot).\n",
    "Slaves\n",
    "In Greek society, slaves were seen as a necessary and perfectly normal part of city-life. Acquired through war and conquest, kidnap and purchase, slaves were simply amongst life’s losers. There were even intellectual arguments from philosophers like Aristotle, which propounded the belief that slaves were demonstrably inferior, a product of their environment and inherited characteristics. Greeks persuaded themselves that it was they who had the best environment and characteristics and the purest blood line and were, therefore, born to rule.\n",
    "It is impossible to say with accuracy how many slaves (douloi) there were in Greek society and what proportion of the population they made up. It is unlikely, due to the costs, that every single citizen had their own slave but some citizens undoubtedly owned many slaves. Accordingly, estimates of the slave population in the Greek world range from between 15 and 40% of the total population. However, a defense speech made in a court case in Athens by Lysias, and hints from others such as Demosthenes, strongly suggest that if every citizen did not have slaves then they certainly desired them, and to be a slave owner was considered a measure of social status. Slaves were not only owned by private individuals but also by the state, which used them in municipal projects such as mining or, as in the case of Athens, the police force.\n",
    "The interactions between slaves and owners was similar to the slave-owner relationships at other points in history; there was a mix of contempt, distrust, and abuse from the owners and contempt, theft, and sabotage from the enslaved. Source material is always from the viewpoint of the slave owner but there are references in literature, particularly in Greek comedy, of friendship and loyalty in at least some owner-slave relationships. Whilst the flogging of slaves is commonly referred to in Greek plays, there were also treatises written extolling the benefits of kindness and incentives in slave management.\n",
    "Slaves worked in all spheres and over 200 occupations have been identified. These include working in the home, in agriculture, industry workshops (e.g.: making shields, food, clothes and perfumes), mines, transport, retail, banking, entertainment, in the armed forces as attendants to their owner or as baggage carriers, as rowers in naval vessels or even as fighters. Farms were generally small affairs with even the richest citizens tending to own several small farms rather than one large estate, therefore, slaves were not concentrated into large groups as in later ancient societies.\n",
    "For slaves there was — at least for some — a glimmer of hope to one day achieve their freedom. There are instances when slaves, particularly those involved in manufacturing and industry, living separately from their owners and given a certain financial independence, could pay for their freedom with money they had saved. Also, slaves in the army were sometimes given their freedom by the state following their victorious exploits.\n",
    "Foreigners\n",
    "Aside from slaves, most Greek poleis would have had a number of free foreigners (xenoi) who had chosen to re-locate from other areas of Greece, the Mediterranean, and the Near East, bringing with them skills such as pottery and metalworking. These foreigners usually had to register their residence and so became a recognized class (lower in status than the full-citizens) called the metics (metoikoi). In return for the benefits of “guest” citizenship they had to provide a local sponsor, pay local taxes, sometimes pay additional taxes, contribute to the costs of minor festivals, and even participate in military campaigns when necessary. Despite the suspicions and prejudices against foreign “barbarians” which often crop up in literary sources, there were cases when metoikoi did manage to become full citizens after a suitable display of loyalty and contribution to the good of the host state. They then received equal tax status and the right to own property and land. Their children too could also become citizens. However, some states, notably Sparta, at times actively discouraged immigration or periodically expelled xenoi. The relationship between foreigners and local citizens seems to have been a strained one, particularly in times of wars and economic hardship.\"\"\",\n",
    "'https://www.commonlit.org/texts/excerpt-from-the-jungle': \"\"\" With one member trimming beef in a cannery, and another working in a sausage factory, the family had a first-hand knowledge of the great majority of Packingtown swindles. For it was the custom, as they found, whenever meat was so spoiled that it could not be used for anything else, either to can it or else to chop it up into sausage. With what had been told them by Jonas, who had worked in the pickle rooms, they could now study the whole of the spoiled-meat industry on the inside, and read a new and grim meaning into that old Packingtown jest—that they use everything of the pig except the squeal.\n",
    "Jonas had told them how the meat that was taken out of pickle would often be found sour, and how they would rub it up with soda to take away the smell, and sell it to be eaten on free-lunch counters; also of all the miracles of chemistry which they performed giving to any sort of meat, fresh or salted, whole or chopped, any color and any flavor and any odor they chose. In the pickling of hams they had an ingenious apparatus, by which they saved time and increased the capacity of the plant—a machine consisting of a hollow needle attached to a pump; by plunging this needle into the meat and working with his foot, a man could fill a ham with pickle in a few seconds. And yet, in spite of this, there would be hams found spoiled, some of them with an odor so bad that a man could hardly bear to be in the room with them. To pump into these the packers had a second and much stronger pickle which destroyed the odor—a process known to the workers as “giving them thirty per cent.” Also, after the hams had been smoked, there would be found some that had gone to the bad. Formerly these had been sold as “Number Three Grade,” but later on some ingenious person had hit upon a new device, and now they would extract the bone, about which the bad part generally lay, and insert in the hole a white-hot iron. After this invention there was no longer Number One, Two, and Three Grade—there was only Number One Grade. The packers were always originating such schemes—they had what they called “boneless hams,” which were all the odds and ends of pork stuffed into casings; and “California hams,” which were the shoulders, with big knuckle joints, and nearly all the meat cut out; and fancy “skinned hams,” which were made of the oldest hogs, whose skins were so heavy and coarse that no one would buy them—that is, until they had been cooked and chopped fine and labeled “head cheese!”\n",
    "It was only when the whole ham was spoiled that it came into the department of Elzbieta. Cut up by the two-thousand-revolutions- a-minute flyers, and mixed with half a ton of other meat, no odor that ever was in a ham could make any difference. There was never the least attention paid to what was cut up for sausage; there would come all the way back from Europe old sausage that had been rejected, and that was moldy and white – it would be dosed with borax and glycerin, and dumped into the hoppers, and made over again for home consumption.\n",
    "There would be meat that had tumbled out on the floor, in the dirt and sawdust, where the workers had tramped and spit uncounted billions of consumption germs. There would be meat stored in great piles in rooms; and the water from leaky roofs would drip over it, and thousands of rats would race about on it. It was too dark in these storage places to see well, but a man could run his hand over these piles of meat and sweep off handfuls of the dried dung of rats. These rats were nuisances, and the packers would put poisoned bread out for them; they would die, and then rats, bread, and meat would go into the hoppers together. This is no fairy story and no joke; the meat would be shoveled into carts, and the man who did the shoveling would not trouble to lift out a rat even when he saw one – there were things that went into the sausage in comparison with which a poisoned rat was a tidbit.\n",
    "There was no place for the men to wash their hands before they ate their dinner, and so they made a practice of washing them in the water that was to be ladled into the sausage. There were the butt-ends of smoked meat, and the scraps of corned beef, and all the odds and ends of the waste of the plants, that would be dumped into old barrels in the cellar and left there. Under the system of rigid economy which the packers enforced, there were some jobs that it only paid to do once in a long time, and among these was the cleaning out of the waste barrels. Every spring they did it; and in the barrels would be dirt and rust and old nails and stale water – and cartload after cartload of it would be taken up and dumped into the hoppers with fresh meat, and sent out to the public's breakfast. Some of it they would make into \"smoked\" sausage – but as the smoking took time, and was therefore expensive, they would call upon their chemistry department, and preserve it with borax and color it with gelatin to make it brown. All of their sausage came out of the same bowl, but when they came to wrap it they would stamp some of it \"special,\" and for this they would charge two cents more a pound.\"\"\",\n",
    "'https://www.commonlit.org/texts/the-third-wave': \"\"\"Background\n",
    "The Third Wave experiment took place at Cubberley High School in Palo Alto, California during the first week of April 1967. History teacher Ron Jones, finding himself unable to explain to his students how people throughout history followed the crowd even when terrible things were happening, decided to demonstrate it to his students through an experiment. Jones announced that he was starting a movement aimed to eliminate democracy. Jones named the movement “The Third Wave” as a symbol of strength, referring to the mythical belief that the third in a series of waves is the strongest. One of the central points of this movement was that democracy’s main weakness is that it favors the individual over the whole community. Jones emphasized this main point of the movement when he created this catchy motto: “Strength through discipline, strength through community, strength through action, strength through pride.”\n",
    "The Experiment\n",
    "Jones started the first day of the experiment emphasizing simple things like proper seating, and drilled the students extensively until they got it right. He then proceeded to enforce strict classroom discipline by emerging as an authoritarian figure. This resulted in dramatic improvements to the efficiency, or orderliness, of the class. The first day’s session ended with only a few rules. Jones intended it to be a one-day experiment. Students had to be sitting at attention before the second bell, had to stand up to ask or answer questions and had to do it in three words or fewer, and were required to preface each remark with “Mr. Jones.” As the week went on, Jones’ class transformed into a group with a supreme sense of discipline and community. Jones made up a salute resembling that of the Nazi regime and ordered class members to salute each other even outside the class. They all obeyed this command.\n",
    "After only three days, the experiment took on a life of its own, with students from all over the school joining in. The class expanded from initial 30 students to 43 attendees. All of the students showed drastic improvement in their academic skills and tremendous motivation. All of the students were issued a member card and each of them received a special assignment, like designing a Third Wave Banner, stopping non-members from entering the class, or other tasks to bring honor to the movement. Jones instructed the students on how to initiate new members, and by the end of the day the movement had over 200 participants. Jones was surprised that some of the students started reporting to him when other members of the movement failed to abide by the rules.\n",
    "By the fourth day of the experiment, the students became increasingly involved in the project and their discipline and loyalty to the project was so outstanding that Jones felt it was slipping out of control. He decided to terminate the movement, so he lied to students by announcing that the Third Wave was a part of a nationwide movement and that on the next day a presidential candidate of the movement would publicly announce its existence on television. Jones ordered students to attend a noon rally on Friday to witness the announcement.\n",
    "At the end of the week, instead of a televised address of their leader, the students were presented with a blank channel. After a few minutes of waiting, Jones announced that they had been a part of an experiment to demonstrate how people willingly create a sense of superiority over others, and how this can lead people to justify doing horrible things in the name of the state’s honor.\"\"\",\n",
    "'https://www.commonlit.org/texts/the-nature-vs-nurture-debate': \"\"\"Background\n",
    "The nature versus nurture debate is about which part of a person is more important—their inherited qualities, which includes genes, or their personal experiences and the way they were brought up. For years, scientists have tried to find out what causes people to have different personalities, behaviors, and characteristics.\n",
    "Scientific Approach\n",
    "The nature versus nurture debate poses a very complicated scientific question: how do scientists figure out which one plays a greater role in the development of a person? Many scientists study twins that have separated at birth to answer this research question. Scientists use twins because they have identical genes, so it makes it easier to observe the effect of a person’s environment. Do you think twins raised in different homes will grow up to have the same personalities?\n",
    "The Nurture Side\n",
    "The view that humans acquire all or almost all of their behavioral traits from “nurture” was termed tabula rasa, Latin for “blank slate,” by philosopher John Locke. This idea proposes that humans develop only from environmental influences. One example of a person’s trait that is completely determined by their environment is native language. Studies show that children, regardless of where they're born, can learn any language with equal facility.\n",
    "The term “nurture” has historically been defined as the care given to children by the parents, with the mother playing an important role. Now, this term is regarded by some as the environmental (non-genetic) factor of a person’s environment. This new definition of “nurture” has been expanded to include, not just a person’s family upbringing, but also everything else they experience in daily life including advertisements, media, education, peer influences, and home environments.\n",
    "The Nature Side\n",
    "Some scientists have concluded that a person’s nature—meaning the traits they got from their parents—have more power in determining a person’s identity than how they are nurtured, or raised. Inherited traits are traits that are developed before birth.\n",
    "Some genetic traits are highly heritable, such as eye color. Some disorders or diseases are also heritable. However, environments are still influential in how that disease affects a person’s life; for example, people who are born with a disease may live a long time depending on how they are cared for. There are also some non-genetic factors that are highly heritable. For example, a wealth and social status are two non-genetic factors that are generally passed down from family.\n",
    "Conclusions\n",
    "Some people criticize the whole concept of “nature versus nurture.” They claim that it is an overly simple way to think about a person’s identity. Perhaps the answer is not as simple. Are there other factors beyond genes and environment that make us who we are?\"\"\",\n",
    "\"https://www.commonlit.org/texts/egyptian-social-structure\": \"\"\"Egyptian society was structured like a pyramid. At the top were the gods, such as Ra, Osiris, and Isis. Egyptians believed that the gods controlled the universe. Therefore, it was important to keep them happy. They could make the Nile overflow, cause famine, or even bring death.\n",
    "The Egyptians also elevated some human beings to gods. Their leaders, called pharaohs, were believed to be gods in human form. They had absolute power over their subjects. After pharaohs died, huge stone pyramids were built as their tombs. Pharaohs were buried in chambers within the pyramids.\n",
    "Because the people of Egypt believed that their pharaohs were gods, they entrusted their rulers with many responsibilities. Protection was at the top of the list. The pharaoh directed the army in case of a foreign threat or an internal conflict. All laws were enacted at the discretion of the pharaoh. Each farmer paid taxes in the form of grains, which were stored in the pharaoh’s warehouses. This grain was used to feed the people in the event of a famine.\n",
    "The Chain of Command\n",
    "No single person could manage all these duties without assistance. The pharaoh appointed a chief minister called a vizier as a supervisor. The vizier ensured that taxes were collected.\n",
    "Working with the vizier were scribes who kept government records. These high-level employees had mastered a rare skill in ancient Egypt — they could read and write.\n",
    "Noble Aims\n",
    "Right below the pharaoh in status were powerful nobles and priests. Only nobles could hold government posts; in these positions they profited from tributes paid to the pharaoh. Priests were responsible for pleasing the gods.\n",
    "Nobles enjoyed great status and also grew wealthy from donations to the gods. All Egyptians — from pharaohs to farmers — gave gifts to the gods.\n",
    "Soldier On\n",
    "Soldiers fought in wars or quelled domestic uprisings. During long periods of peace, soldiers also supervised the peasants, farmers, and slaves who were involved in building such structures as pyramids and palaces.\n",
    "Skilled workers such as physicians and craftsmen/women made up the middle class. Craftsmen made and sold jewelry, pottery, papyrus products, tools, and other useful things.\n",
    "Naturally, there were people needed to buy goods from artisans and traders. These were the merchants and storekeepers who sold these goods to the public.\n",
    "The Bottom of the Heap\n",
    "At the bottom of the social structure were slaves and farmers. Slavery became the fate of those captured as prisoners of war. In addition to being forced to work on building projects, slaves toiled at the discretion of the pharaoh or nobles.\n",
    "Farmers tended the fields, raised animals, kept canals and reservoirs in good order, worked in the stone quarries, and built the royal monuments. Farmers paid taxes that could amount to as much as 60% of their yearly harvest—that’s a lot of hay!\n",
    "Social mobility was not impossible. A small number of peasants and farmers moved up the economic ladder. Families saved money to send their sons to village schools to learn trades. These schools were run by priests or by artisans. Boys who learned to read and write could become scribes, then go on to gain employment in the government. It was possible for a boy born on a farm to work his way up into the higher ranks of the government. Bureaucracy proved lucrative.\"\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d32b7c56-2d25-4f57-8c51-ccef8593d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = df['prompt_url'].apply(lambda x: source_texts[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "163a6869-fdf4-4d0b-89b5-3b0ce6053248",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['response']+'</s>'+df['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d495469-35f1-42a6-ba07-5aa1fd6aaa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset, Value, ClassLabel, Features, DatasetDict\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification, LongformerConfig\n",
    "from transformers import DataCollatorForLanguageModeling, LongformerForMaskedLM\n",
    "\n",
    "import torch\n",
    "seed = 42\n",
    "max_length = 4096\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name, model_max_length = max_length)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "\n",
    "def tokenizeForMLM(batch):\n",
    "    return tokenizer(batch['text'], return_special_tokens_mask = True, truncation=True)\n",
    "\n",
    "mlm_df = df[['text']].dropna()\n",
    "ds = Dataset.from_pandas(mlm_df, preserve_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "700156fd-fcd6-4045-a910-6ed12d02fecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100368b150b742e799e2eef0d4cdc1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac81e3abf70c46ac97dc499f4987ad40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa86a9929b3455887c73a7dcdf30577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlm_ds = ds.map(tokenizeForMLM, batched=True)\n",
    "\n",
    "# this generates labels by copying the input ids\n",
    "def group_texts(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "mlm_ds = mlm_ds.map(group_texts, batched=True)\n",
    "\n",
    "# this generates labels by copying the input ids\n",
    "def group_texts(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "mlm_ds = mlm_ds.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10c61917-b2b8-4782-917c-031c2b1ead2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'special_tokens_mask', 'attention_mask', 'labels'],\n",
       "        num_rows: 78543\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'input_ids', 'special_tokens_mask', 'attention_mask', 'labels'],\n",
       "        num_rows: 13861\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate train and valid sets\n",
    "def buildDataset(ds):\n",
    "    full_dataset = ds\n",
    "    # 70% train, 30% test\n",
    "    train_valid = full_dataset.train_test_split(test_size=0.15, seed=seed)\n",
    "    # gather everyone if you want to have a single DatasetDict\n",
    "    final_dataset = DatasetDict({\n",
    "        'train': train_valid['train'],\n",
    "        'valid': train_valid['test']})\n",
    "    return final_dataset\n",
    "mlm_ds = buildDataset(mlm_ds)\n",
    "mlm_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2ad4d85d-2deb-4863-8de9-a00a3bca6578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series([len(seq) for seq in mlm_ds['train']['input_ids']]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c48a26c-8965-4321-a563-717ec371cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /home/jovyan/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
      "Model config LongformerConfig {\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/allenai/longformer-base-4096/resolve/main/pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/transformers/a7a586602e625bd012d75abdfcc615f5bb1fe133273845f7381332c634273bd9.dc3a4f03d4ab11f972b126d0e6b67f43e5d9003b3aec54f8e549cc7e2d42398d\n",
      "All model checkpoint weights were used when initializing LongformerForMaskedLM.\n",
      "\n",
      "Some weights of LongformerForMaskedLM were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, mlm_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m) \n\u001b[1;32m     19\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     20\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./longformer_da_results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlm_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlm_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:332\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_init \u001b[38;5;241m=\u001b[39m model_init\n\u001b[0;32m--> 332\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_model_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer` requires either a `model` or `model_init` argument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1171\u001b[0m, in \u001b[0;36mTrainer.call_model_init\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m   1169\u001b[0m model_init_argcount \u001b[38;5;241m=\u001b[39m number_of_arguments(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_init)\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_init_argcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1171\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_init_argcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1173\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_init(trial)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mmodel_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_init\u001b[39m():\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLongformerForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# Show the training loss with every epoch\n",
    "\n",
    "import torch\n",
    "seed = 42\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "#torch.cuda.empty_cache()\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    return LongformerForMaskedLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15) \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./longformer_da_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=mlm_ds[\"train\"],\n",
    "    eval_dataset=mlm_ds[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95803966-cb79-4eca-8a78-f0a229282416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
