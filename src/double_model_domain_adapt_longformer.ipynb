{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9911c7cc-39bc-4ae3-b053-d36b9d15b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6360b383-652f-4aa6-9bad-f8c36ddba5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path.cwd().parent / 'data'\n",
    "SUMM_FOLDER = DATA / 'summaries_finetune'\n",
    "TEXT_FILES = SUMM_FOLDER / 'text_files_copy'\n",
    "SOURCE_TEXTS = SUMM_FOLDER / 'source_texts_clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5ae0159-7cec-42d0-8f15-92034ab94d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(DATA / 'source_dict.txt', 'r')\n",
    "source_dict = json.loads(data.read())\n",
    "source_texts = list(source_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "40014648-847b-4577-9f36-f61fdc383bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df = pd.read_csv(SUMM_FOLDER / 'final_summaries_ai_aloe_fixed.csv')[['text','source', 'paraphrase_pca', 'content_pca', 'source_text_filename_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f9d21d-2046-4514-9f28-fd88d431e51c",
   "metadata": {},
   "source": [
    "## Getting final hidden embeddings for each summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b36aa2b-b165-414c-ba37-3c2a442714df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import LongformerTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "model_name = \"allenai/longformer-base-4096\" #\"google/bigbird-roberta-base\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name, output_hidden_states=True).to(DEVICE)\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "def getLastState(text):\n",
    "    tokenized_text = tokenizer(text, return_tensors='pt').to(DEVICE)\n",
    "    outputs = model(**tokenized_text)\n",
    "    return outputs.last_hidden_state[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b398d8-f7d2-4c2e-a79f-cc651e28443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df['source_embedding'] = summaries_df['source'].apply(lambda x: getLastState(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5580bb-6647-421a-a7af-2bcc648f36ec",
   "metadata": {},
   "source": [
    "### Normalize Dependent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "574bb84b-5d8c-4b32-9c5c-dc152ae40172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "  \n",
    "# copy the data\n",
    "  \n",
    "# apply normalization techniques\n",
    "summaries_df['content_pca'] = StandardScaler().fit_transform(np.array(summaries_df['content_pca']).reshape(-1,1))\n",
    "summaries_df['paraphrase_pca'] = StandardScaler().fit_transform(np.array(summaries_df['paraphrase_pca']).reshape(-1,1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571e51d7-f80f-4f33-9117-97ec5458a4b6",
   "metadata": {},
   "source": [
    "### Remove Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df59643f-b300-424c-b22f-2137797c58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_texts = summaries_df['source_text_filename_clean'].value_counts().to_frame().reset_index()\n",
    "texts_to_remove = list(source_texts.iloc[15:31]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b499a2b-ebc1-4b00-b261-ce0e3773e21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test n: 703\n",
      "train n: 3987\n"
     ]
    }
   ],
   "source": [
    "test_df = summaries_df[summaries_df['source_text_filename_clean'].isin(texts_to_remove)]\n",
    "train_df = summaries_df[summaries_df['source_text_filename_clean'].isin(texts_to_remove) == False]\n",
    "print('test n:', len(test_df))\n",
    "print('train n:', len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf66eaf-7a52-482e-a57a-152b3326059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataset(df):\n",
    "    full_dataset = Dataset.from_pandas(df, preserve_index=False)\n",
    "    # 70% train, 30% test\n",
    "    train_valid = full_dataset.train_test_split(test_size=0.176, seed=seed)\n",
    "    # gather everyone if you want to have a single DatasetDict\n",
    "    final_dataset = DatasetDict({\n",
    "        'train': train_valid['train'],\n",
    "        'valid': train_valid['test']})\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc7b50-8d9c-45a9-ad62-aa9edbeac610",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df = train_df[['text', 'source_embedding', 'content_pca']]\n",
    "content_df.columns = ['text', 'labels']\n",
    "content_ds = buildDataset(content_df)\n",
    "\n",
    "paraphrase_df = train_df[['text', 'source_embedding', 'paraphrase_pca']]\n",
    "paraphrase_df.columns = ['text', 'source_embedding', 'labels']\n",
    "paraphrase_ds = buildDataset(paraphrase_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2426cd-e333-4fa8-b83f-bddbc3b05298",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_ds['test'] = Dataset.from_pandas(test_df[['text', 'source_embedding', 'content_pca']].rename(columns={'content_pca':'labels'}), preserve_index=False)\n",
    "paraphrase_ds['test'] = Dataset.from_pandas(test_df[['text', 'source_embedding', 'paraphrase_pca']].rename(columns={'paraphrase_pca':'labels'}), preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf76d8-adef-43d8-bd51-08d045052388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_inputs(example):\n",
    "    return tokenizer(example['text'], truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "23b80a49-8a29-4b91-9912-826c5f7d654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = getLastState('here is the text that should be used to color the next model')\n",
    "\n",
    "tokens = tokenizer.encode('</s> this is a test of the emergency broadcast system', return_tensors='pt').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "95803966-cb79-4eca-8a78-f0a229282416",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.embeddings.word_embeddings(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "af88d47b-d417-4575-9bb9-52b00e7a388a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "18213f55-caa7-4893-acb7-72827bdf164b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "789987fe-cb30-4d53-8e8c-faa813d2f3ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [109]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"
     ]
    }
   ],
   "source": [
    "c = torch.cat([a[0], b, a[1:]], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d92de8ae-4c14-415b-94f9-5feea3b9e5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3b6841a0-530a-47e4-b368-e9b14e5cd1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "445690f0-b4eb-42b9-a682-c6f0beddfaad",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [120]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 1"
     ]
    }
   ],
   "source": [
    "torch.cat([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ecbe0e-8022-4ae1-a0aa-b5fa8f8958b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdacloud for cloud inferencing "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
